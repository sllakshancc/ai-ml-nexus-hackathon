{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf283e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pandas numpy scikit-learn joblib streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cuml-cu12 --index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874557b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "def load_first_tabular_file(dataset_name):\n",
    "    # Download the entire dataset (or at least one supported file)\n",
    "    path = kagglehub.dataset_download(dataset_name, force_download=True)\n",
    "    # The `path` returns the directory or file path where data has been placed.\n",
    "\n",
    "    # Then you can manually inspect the downloaded path, search for supported extensions\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    supported_exts = [\".csv\", \".tsv\", \".json\", \".jsonl\", \".parquet\",\n",
    "                      \".xls\", \".xlsx\", \".xlsm\", \".xlsb\", \".ods\"]\n",
    "    # Walk the path to find a suitable file\n",
    "    file_to_load = None\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            _, ext = os.path.splitext(f)\n",
    "            if ext.lower() in supported_exts:\n",
    "                file_to_load = os.path.join(root, f)\n",
    "                break\n",
    "        if file_to_load:\n",
    "            break\n",
    "\n",
    "    if file_to_load is None:\n",
    "        raise ValueError(f\"No supported tabular file found in dataset '{dataset_name}'\")\n",
    "\n",
    "    df = pd.read_csv(file_to_load)  # adjust if needed for e.g. Excel\n",
    "    return df\n",
    "\n",
    "dataset_name = \"shanegerami/ai-vs-human-text\"\n",
    "complete_data_set = load_first_tabular_file(dataset_name)\n",
    "\n",
    "print(\"Columns:\", complete_data_set.columns.tolist())\n",
    "print(\"First 5 records:\")\n",
    "print(complete_data_set.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb30a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete_data_set.sample(n=100, replace=False, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Check columns\n",
    "print(\"Columns in df:\", data.columns.tolist())\n",
    "\n",
    "# Make sure 'generated' and 'text' exist\n",
    "if \"generated\" in data.columns and \"text\" in data.columns:\n",
    "\n",
    "    def get_random_text(label):\n",
    "        \"\"\"\n",
    "        Get a random text for a given label.\n",
    "        label: 0 for human, 1 for AI\n",
    "        \"\"\"\n",
    "        subset = data[data[\"generated\"] == label]\n",
    "        if len(subset) == 0:\n",
    "            return f\"No texts found for label {label}\"\n",
    "        return subset[\"text\"].sample(n=1).iloc[0]\n",
    "\n",
    "    # Example usage\n",
    "    random_human_text = get_random_text(0)\n",
    "    random_ai_text = get_random_text(1)\n",
    "\n",
    "    print(\"Random human text:\", random_human_text)\n",
    "    print(\"\\n\\nRandom AI text:\", random_ai_text)\n",
    "\n",
    "else:\n",
    "    print(\"'generated' or 'text' column not found in the DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfe5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, numpy as np\n",
    "\n",
    "STOP = set(\"\"\"a an the in on at of for to and or but if while as is are was were be been being from with by than then so because about into over after before under between out up down off near far very just not no nor\"\"\".split())\n",
    "\n",
    "_SENT_SPLIT = re.compile(r\"[.!?]+\")\n",
    "_WORD = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "_EMOJI = re.compile(r\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
    "\n",
    "def shannon_entropy(s: str) -> float:\n",
    "    if not s: return 0.0\n",
    "    counts = {}\n",
    "    for ch in s:\n",
    "        counts[ch] = counts.get(ch, 0) + 1\n",
    "    n = len(s)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in counts.values())\n",
    "\n",
    "def sent_lengths(s: str):\n",
    "    sents = [x.strip() for x in _SENT_SPLIT.split(s) if x.strip()]\n",
    "    if not sents: return (0.0, 0.0)\n",
    "    lens = [len(_WORD.findall(x)) for x in sents]\n",
    "    return (float(np.mean(lens)), float(np.var(lens)))\n",
    "\n",
    "def type_token_ratio(words):\n",
    "    if not words: return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def hapax_ratio(words):\n",
    "    if not words: return 0.0\n",
    "    from collections import Counter\n",
    "    c = Counter(words)\n",
    "    hapax = sum(1 for k,v in c.items() if v == 1)\n",
    "    return hapax / len(c)\n",
    "\n",
    "def function_word_ratio(words):\n",
    "    if not words: return 0.0\n",
    "    fw = sum(1 for w in words if w.lower() in STOP)\n",
    "    return fw / len(words)\n",
    "\n",
    "def repetition_index(words, n=3):\n",
    "    # max frequency of any n-gram / total n-grams\n",
    "    if len(words) < n: return 0.0\n",
    "    from collections import Counter\n",
    "    grams = [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    cnt = Counter(grams)\n",
    "    return max(cnt.values()) / max(1, len(grams))\n",
    "\n",
    "def extract_features(text: str) -> np.ndarray:\n",
    "    s = text.strip()\n",
    "    words = _WORD.findall(s)\n",
    "    chars = len(s) or 1\n",
    "    sent_avg, sent_var = sent_lengths(s)\n",
    "\n",
    "    features = [\n",
    "        shannon_entropy(s),                         # char_entropy\n",
    "        type_token_ratio(words),                    # ttr\n",
    "        hapax_ratio(words),                         # hapax\n",
    "        sent_avg,                                   # avg_sent_len\n",
    "        sent_var,                                   # var_sent_len\n",
    "        sum(1 for c in s if c in \".,;:!?\")/chars,   # punct_rate\n",
    "        sum(1 for c in s if c.isdigit())/chars,     # digit_rate\n",
    "        len(_EMOJI.findall(s))/chars,               # emoji_rate\n",
    "        function_word_ratio(words),                 # function_word_ratio\n",
    "        repetition_index([w.lower() for w in words], n=3) # repetition_index\n",
    "    ]\n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"char_entropy\",\"ttr\",\"hapax\",\"avg_sent_len\",\"var_sent_len\",\n",
    "    \"punct_rate\",\"digit_rate\",\"emoji_rate\",\"function_word_ratio\",\"repetition_index\"\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
